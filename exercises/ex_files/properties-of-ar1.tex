Let $\{\varepsilon_t\}$ be a white noise process.

\begin{solution}\textbf{Solution to \nameref{ex:PropertiesAR1}}
\end{solution}

\begin{enumerate}
    \item Consider the first-order autoregressive, i.e. AR(1), model:  $$y_t = \phi y_{t-1} + \varepsilon_{t}$$ Derive the conditional and unconditional first and second moments.
          \begin{solution}
              \begin{itemize}
                  \item Recursive substitution (starting at some infinite time $j$):
                        \begin{align*}
                            y_t & = \phi y_{t-1} + \varepsilon_t                                                                                      \\
                                & = \phi \left( \phi y_{t-2} + \varepsilon_{t-1}\right) + \varepsilon_t                                               \\
                                & = \phi^2(\phi y_{t-3} + \varepsilon_{t-2} ) + \phi \varepsilon_{t-1} + \varepsilon_t                                \\
                                & \vdots                                                                                                              \\
                                & = \phi^{j+1} y_{t-{j+1}}+\phi^j \varepsilon_{t-j}+...+\phi^2 \varepsilon_{t-2} + \phi \varepsilon_t + \varepsilon_t
                        \end{align*}
                        $y_t$ is a linear function of an initial value and historical values of white noise. If $|\phi|<1$ and j becomes large, then $ \phi^{j+1} y_{t-{j+1}} \rightarrow 0$, thus we get a so-called $MA(\infty)$ process:
                        $$y_t = \varepsilon_t + \phi \varepsilon_{t-1} + \phi^2 \varepsilon_{t-2}+... = \sum_{j=0}^\infty \phi^j \varepsilon_{t-j}$$
                  \item With Lag Operators: works only if $|\phi| < 1$ and $\{y_t\}$ is bounded, that is, there exists a finite number $k$ such that $|y_t| < k$ for all t. Then
                        \begin{align*}
                            (1-\phi L) y_t = \varepsilon_t                                    \\
                            (1-\phi L)^{-1}(1-\phi L) y_t =	y_t = (1-\phi L)^{-1}\varepsilon_t \\
                        \end{align*}
                        We can use the geometric rule: $(1-\phi L)^{-1} = \lim\limits_{j\rightarrow \infty}(1+\phi L + \phi^2 L^2+...+(\phi L)^j)$
                        Therefore:$$y_t = \varepsilon_t + \phi \varepsilon_{t-1} + \phi^2 \varepsilon_{t-2}+... = \sum_{j=0}^\infty \phi^j \varepsilon_{t-j}$$

              \end{itemize}
              If we can express an AR process as a MA process, we call this process invertible.\\
              Unconditional Moments:
              \begin{align*}
                  E(y_t)           & = 0                                \\
                  var(y_t)         & = \frac{\sigma^2}{1-\phi}          \\
                  cov(y_t,y_{t-j}) & = \phi^j var(y_t) \text{ for } j>0
              \end{align*}
              Conditional Moments, conditional on $y_{t-1}$:
              \begin{align*}
                  E(y_t|y_{t-1})                         & = \phi y_{t-1}       \\
                  var(y_t|y_{t-1})                       & = \sigma^2           \\
                  cov((y_t|y_{t-1}),(y_{t-j}|y_{t-j-1})) & = 0 \text{ for } j>0
              \end{align*}
              Conditional Moments, conditional on $y_{t-2}$:
              \begin{align*}
                  E(y_t|y_{t-2})                         & = \phi^2 y_{t-2}                \\
                  var(y_t|y_{t-2})                       & = (1+\phi^2)\sigma^2            \\
                  cov((y_t|y_{t-2}),(y_{t-j}|y_{t-j-2})) & = \phi\sigma^2 \text{ for } j=1 \\
                  cov((y_t|y_{t-2}),(y_{t-j}|y_{t-2}))   & = 0 \text{ for } j>1
              \end{align*}

          \end{solution}
    \item  Simulate different AR(1) processes and plot the corresponding autocorrelation function (ACF). To this end write a function \texttt{ACFPlots($y$,$p^{max}$,$\alpha$)} to plot the ACF of the data vector $y$ with maximum number of lags $p^{max}$. Also include an approximate (1-$\alpha$)\% confidence interval around zero in your plot. Hints:
          \begin{itemize}
              \item The empirical autocorrelation function at lag $k$ is defined as $r_k = c_k/c_0$ where
                    $$c_k = \frac{1}{T} \sum_{t=k+1}^T (y_t - \bar{y})(y_{t-k}-\bar{y})$$ and $$c_0 = \frac{1}{T} \sum_{t=1}^T (y_t - \bar{y})(y_{t}-\bar{y})$$
                    You can either use a for-loop to compute the sum or use vectors: $(y - \bar{y})'(y - \bar{y})$.
              \item The sample autocorrelations are estimates of the actual autocorrelations if the process is stationary. If it is purely random, that is, all members are mutually independent and identically distributed so that $y_t$  and $y_{t-k}$ are stochastically independent for $k\neq 0$, then the normalized estimated autocorrelations are asymptotically standard normally distributed, i.e. $\sqrt{T} r_k \rightarrow U \sim N(0,1)$ and thus $r_k \rightarrow \tilde{U} \sim N(0,1/T)$.
          \end{itemize}

          \begin{solution}
              The ACF is a useful plot to understand the dynamics of a process:
              \lstinputlisting{../progs/ACFPlots.m}
          \end{solution}
\end{enumerate}
