Let $y_t$ be a K-dimensional time series and $u_t$ a K-dimensional white noise process.
\begin{enumerate}
    \item Why are we concerned with multivariate time series? For example, why do we model the VAR(1) process
          \begin{align*}
              \begin{pmatrix}y_{1,t}\\ y_{2,t}\end{pmatrix} = \begin{pmatrix} c_1 \\ c_2\end{pmatrix} + \begin{pmatrix}a_{11} & a_{12}\\ a_{21} & a_{22} \end{pmatrix} \begin{pmatrix}y_{1,{t-1}}\\ y_{2,{t-1}}\end{pmatrix} + \begin{pmatrix}u_{1,t}\\ u_{2,t}\end{pmatrix}
          \end{align*}
          simultaneously instead of two models for each variable separately?
          \begin{solution}
              For the specification of multi-equation models we require a clear distinction between exogenous and endogenous variables. In economic theory this is often not clear or arbitrarily made in practice. Vectorautoregressive models do not need this distinction, they can rather be understood as a dynamic version of a simultaneous multi-equation model. This corresponds to reality, because economic variables are generated by dynamic processes, and often are interdependent. VAR models therefore provide a powerful instrument. They also take into account things like non-stationarity (cointegration and long-term equilibria), as well as the analysis of dynamics of random shocks / impulses. An Example: we are also able to consider correlations between $ u_ {1, t} $ and $ u_ {2, t} $. Lastly, VAR models tend to have better predictive power than multi-equation models and are often used as a benchmark for different forecasting models.

          \end{solution}
    \item Interpret $E[y_t]$ and $\Gamma_h := Cov[y_{t},y_{t-h}]$.
          \begin{solution}
              \emph{Interpretation von $E[y_t]$:} Jede Komponente von $y_t$ hat einen eigenen Erwartungswert. $E[y_t]$ ist somit der unbedingte Erwartungswert von $y_t$ zum Zeitpunkt $t$. Der Erwartungswert als linearer Operator l\"{a}sst sich in einen Vektor oder in eine Matrix hineinziehen:
              \begin{align*}
                  E[y_t] = E\left[\begin{pmatrix}y_{1,t} \cr \vdots \cr y_{K,t} \end{pmatrix} \right] = \begin{pmatrix}E[y_{1,t}] \cr \vdots \cr E[y_{K,t}] \end{pmatrix}
              \end{align*}
              \emph{Interpretation of $\Gamma_y(h)$:} In the univariate case we defined the autocovariance as the covariance of a random variable with its own lagged values. In the multivariate case we also consider covariances between different variables. To this end, we want to summarize the covariance between different variables and different points in time. The Autocovariance matrix ${\Gamma_y(h)} := Cov[{y_{t},{y_{t-h}}}]$ sums this information up in a neat fashion and is therefore a powerful tool in multivariate time series analysis:
              \begin{align*}
                   & Cov[{y_{t},{y_{t-h}}}] = E\left[({y_{t}}-E[{{y_{t}}}]) ({y_{t-h}}-E[{{y_{t-h}}}])' \right]                                                                                                                                                                                                                                     \\
                   & = E\left[\begin{pmatrix}y_{1,t} - E[y_{1,t}] \cr \vdots \cr y_{K,t} - E[y_{K,t}] \end{pmatrix} \begin{pmatrix} y_{1,{t-h}} - E[y_{1,{t-h}}] & \dots & y_{K,{t-h}} - E[y_{K,{t-h}}]  \end{pmatrix} \right] \\
                   & = E\left[\begin{pmatrix}
                                      (y_{1,t}- E[y_{1,t}])(y_{1,{t-h}} - E[y_{1,{t-h}}]) & \dots  & (y_{1,t} - E[y_{1,t}])(y_{K,{t-h}} - E[y_{K,{t-h}}]) \\
                                      \vdots                                              & \ddots & \vdots                                               \\
                                      (y_{K,t}- E[y_{K,t}])(y_{1,{t-h}} - E[y_{1,{t-h}}]) & \dots  & (y_{K,t} - E[y_{K,t}])(y_{K,{t-h}} - E[y_{K,{t-h}}])
                                  \end{pmatrix} \right]                                           \\
                   & = \begin{pmatrix}
                           E[(y_{1,t}- E[y_{1,t}])(y_{1,{t-h}} - E[y_{1,{t-h}}])] & \dots  & E[(y_{1,t} - E[y_{1,t}])(y_{K,{t-h}} - E[y_{K,{t-h}}])] \\
                           \vdots                                                 & \ddots & \vdots                                                  \\
                           E[(y_{K,t}- E[y_{K,t}])(y_{1,{t-h}} - E[y_{1,{t-h}}])] & \dots  & E[(y_{K,t} - E[y_{K,t}])(y_{K,{t-h}} - E[y_{K,{t-h}}])]
                       \end{pmatrix}                                        \\
                   & =\begin{pmatrix}
                          Cov(y_{1,t},y_{1,{t-h}}) & \dots  & Cov(y_{1,t},y_{K,{t-h}}) \\
                          \vdots                   & \ddots & \vdots                   \\
                          Cov(y_{K,t},y_{1,{t-h}}) & \dots  & Cov(y_{K,t},y_{K,{t-h}})
                      \end{pmatrix}
              \end{align*}
              On the diagonals we have the autocovariance of each variable, on the off-diagonals we have the covariances between the different variables.
          \end{solution}
    \item How does the definition of covariance stationary change in the multivariate case?
          \begin{solution}
              Basically it is the same: A stochastic process is called weakly stationary (or covariance stationary), if for each period in time it has the same expectation independent of time and the autocovariance between to points in time is only dependent on the distance of these two points. In the multivariate case we now also consider the autocovariances between different variables and require these to be only dependent on the distance in time, not on time itself.\\
              \texttt{Definition}: The K-dimensional stochastic process $y_t$ is called covariance stationary, if for all $h,t,\tau \in \mathbb{Z}$:
              \begin{align}
                  E[{y_t}]           & = E[{y_\tau}]                                  \\
                  Cov(y_{t},y_{t-h}) & = Cov(y_{t+\tau}, y_{t-h+\tau}) \label{gammas}
              \end{align}
          \end{solution}
    \item Consider the following VAR(1) process $y_t = c + A y_{t-1} + u_t$
          \begin{align*}
              \begin{pmatrix}y_{1,t} \\ y_{2,t}\\y_{3,t} \end{pmatrix} & = \begin{pmatrix}0\\0\\0 \end{pmatrix} + \begin{pmatrix}0.5 &0 &0 \\0.1&0.1&0.3\\0&0.2&0.3 \end{pmatrix} \begin{pmatrix}y_{1,{t-1}} \\ y_{2,{t-1}}\\y_{3,{t-1}} \end{pmatrix} + \begin{pmatrix}u_{1,t} \\ u_{2,t}\\ u_{3,t} \end{pmatrix}
          \end{align*}
          provided that $u_t \sim WN(0,\Sigma_u)$ and $$\Sigma_u = \begin{pmatrix}2.25 & 0 & 0\\ 0 & 1 & 0.5\\ 0 & 0.5 & 0.74 \end{pmatrix}$$
          Compute the coefficients $\Phi_0, \Phi_1, \dots \in \mathbb{R}^{3\times 3}$ of the lag polynomial $\Phi(L) := \sum_{i=0}^\infty \Phi_i L^i$, and a $\nu \in \mathbb{R}^3$ such that
          \begin{align*}
              y_t = \nu + \Phi(L) u_t
          \end{align*}
          \begin{solution}
              We will use the very efficient method of matching coefficients to transform the VAR(1) model into a VMA($\infty$) representation.
              \begin{align*}
                  y_t                                 & = A y_{t-1} + u_t                   \\
                  \underbrace{(I_3 - A L)}_{A(L)} y_t & = u_t                               \\
                  A(L) y_t                            & = u_t                               \\
                  A(L)^{-1}A(L) y_t                   & = A(L)^{-1} u_t                     \\
                  y_t                                 & = A(L)^{-1} u_t = \nu + \Phi(L) u_t \\
                  \Rightarrow                         & \nu= 0, \Phi(L) = A(L)^{-1}
              \end{align*}
              In the method of matching coefficient we compare the coefficient matrices multiplied to each power of $L$. That is, the expression on the left hand side has to match the expression on the right hand side. In our case:
              \begin{align*}
                  \Phi(L)                                               & = A(L)^{-1} \\
                  A(L) \Phi(L)                                          & = I_3       \\
                  (I_3 - A L) \left(\sum_{i=0}^\infty \Phi_i L^i\right) & = I_3
              \end{align*}
              \begin{align*}
                  \Phi_0 L^0 & + \Phi_1 L^1 + \Phi_2 L^2 + \dots               \\
                             & - A \phi_0 L^1 - A \phi_1 L^2 - \dots = I_3 L^0
              \end{align*}
              \begin{align}
                  L^0 & : \Phi_0                                     & = I_3 \Rightarrow & \Phi_0 = I_3 = \begin{pmatrix}1&0&0\\0&1&0\\0&0&1 \end{pmatrix}\nonumber                                 \\
                  L^1 & : \Phi_1 - A \Phi_0                          & = 0 \Rightarrow   & \Phi_1 = A \Phi_0 = A = \begin{pmatrix}0.5&0&0\\0.1&0.1&0.3\\0&0.2&0.3 \end{pmatrix}\nonumber            \\
                  L^2 & : \Phi_2 - A \Phi_1                          & = 0 \Rightarrow   & \Phi_2 = A \Phi_1 = A^2 = \begin{pmatrix}0.25&0&0\\0.06&0.07&0.12\\0.02&0.08&0.15 \end{pmatrix}\nonumber \\
                  \vdots\nonumber                                                                                                                                                                   \\
                      & \text{In general}: \Phi_s = A^s \label{allg}
              \end{align}
          \end{solution}
\end{enumerate}


\newpage
